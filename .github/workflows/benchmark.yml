name: LiveCodeBench

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:
    inputs:
      sample_size:
        description: 'Number of problems (0 = full)'
        default: '100'
        type: string
      model:
        description: 'Model to benchmark'
        default: 'claude-opus-4-5-20251101'
        type: string

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r scripts/requirements.txt

      - name: Run benchmark
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          BENCHMARK_MODEL: ${{ inputs.model || 'claude-opus-4-5-20251101' }}
          BENCHMARK_SAMPLE_SIZE: ${{ inputs.sample_size || '100' }}
        run: |
          python scripts/run_benchmark.py \
            --model "$BENCHMARK_MODEL" \
            --sample "$BENCHMARK_SAMPLE_SIZE"

      - name: Upload results to API
        env:
          ADMIN_API_KEY: ${{ secrets.ADMIN_API_KEY }}
        run: |
          curl -X POST "https://benchmarks.emilycogsdill.com/api/results" \
            -H "Authorization: Bearer $ADMIN_API_KEY" \
            -H "Content-Type: application/json" \
            -d @results.json \
            --fail-with-body

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: results.json
          retention-days: 30
